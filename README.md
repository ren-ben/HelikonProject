# Helikon - CLIL Material Generator

Ein Tool zur Erstellung von CLIL-Unterrichtsmaterialien (Content and Language Integrated Learning) mit Unterst√ºtzung f√ºr lokale LLM-Modelle.

## Features

- ü§ñ **Lokale LLM-Integration**: Verwendet Ollama f√ºr lokal installierte Sprachmodelle
- üìù **5-Schritt Material-Erstellung**: Gef√ºhrter Workflow f√ºr die Material-Generierung
- üéØ **CLIL-fokussiert**: Spezialisiert auf bilingualen Fachunterricht
- üíæ **Material-Verwaltung**: Speichern, Bearbeiten und Exportieren von Materialien
- üé® **Rich-Text Editor**: TipTap-basierter Editor mit Formatierungsoptionen
- üåç **Mehrsprachig**: Unterst√ºtzung f√ºr Deutsch, Englisch und technische F√§cher

## Technologie-Stack

### Backend
- Java 21
- Spring Boot 3.2.3
- PostgreSQL 15
- Ollama API Integration

### Frontend
- Vue.js 3
- Vuetify 3
- Vite
- Pinia (State Management)

## Voraussetzungen

- Java 21 oder h√∂her
- Node.js 18 oder h√∂her
- Docker & Docker Compose
- Ollama (mit installierten Modellen)

## Installation & Setup

### 1. Ollama installieren

```bash
# macOS
brew install ollama

# Modelle installieren
ollama pull llama3.2
ollama pull gemma2:9b
ollama pull deepseek-r1:8b
```

### 2. Datenbank starten

```bash
docker-compose up -d
```

### 3. Backend starten

```bash
./mvnw spring-boot:run
```

Das Backend l√§uft auf: `http://localhost:8081`

### 4. Frontend starten

```bash
cd clil-frontend
npm install
npm run dev
```

Das Frontend l√§uft auf: `http://localhost:5173`

## Verwendung

1. √ñffne `http://localhost:5173` im Browser
2. Klicke auf "Neues Material erstellen"
3. Folge dem 5-Schritt-Prozess:
   - **Schritt 1**: Material-Typ ausw√§hlen (Arbeitsblatt, Quiz, etc.)
   - **Schritt 2**: Fach und Thema angeben
   - **Schritt 3**: CLIL-Parameter festlegen (Sprachniveau, Vokabular-Anteil)
   - **Schritt 4**: LLM-Modell ausw√§hlen und Prompt anpassen
   - **Schritt 5**: Material generieren

## Verf√ºgbare Modelle

Das System l√§dt automatisch alle lokal installierten Ollama-Modelle. Um neue Modelle hinzuzuf√ºgen:

```bash
ollama pull <model-name>
```

Beispiele:
- `llama3.2` - Schnelles Allzweck-Modell
- `gemma2:9b` - Gr√∂√üeres Modell f√ºr komplexe Aufgaben
- `mistral` - Optimiert f√ºr deutsche Texte

## API Endpoints

- `GET /api/v1/clil/models` - Liste aller verf√ºgbaren lokalen Modelle
- `POST /api/v1/clil/generate` - Material generieren
- `GET /api/v1/clil/materials` - Alle gespeicherten Materialien
- `GET /api/v1/clil/materials/{id}` - Einzelnes Material abrufen
- `POST /api/v1/clil/materials` - Material speichern
- `PUT /api/v1/clil/materials/{id}` - Material aktualisieren
- `DELETE /api/v1/clil/materials/{id}` - Material l√∂schen

## Konfiguration

Die Hauptkonfiguration befindet sich in `src/main/resources/application.properties`:

```properties
# Server
server.port=8081

# Ollama API
ollama.api.url=http://localhost:11434
ollama.api.default-model=llama3.2

# Timeouts (3 Minuten f√ºr lokale Modelle)
spring.mvc.async.request-timeout=180000
```

## Hinweise

- Die Generierung mit lokalen Modellen kann je nach Hardware 1-3 Minuten dauern
- Gr√∂√üere Modelle (>7B Parameter) ben√∂tigen mehr Zeit und Ressourcen
- Die Qualit√§t der Ergebnisse variiert je nach gew√§hltem Modell

## Lizenz

Dieses Projekt ist f√ºr den akademischen und p√§dagogischen Einsatz gedacht
